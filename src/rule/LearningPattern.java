package rule;

import java.util.ArrayList;

import entity.Individual;
import utils.RandomUtil;

public enum LearningPattern {
	/** MAXPAYOFF：从直接邻居学习策略，个体学习具有最大累积收益的邻居 */
	MAXPAYOFF("max_payoff_learning") {
		@Override
		public float learn(Individual in) {
			Individual maxPayoffNeighbour = in;
			final ArrayList<Individual> neighbours = in.getNeighbours();
			for (Individual nei : neighbours) {
				if (nei.getAccumulatedPayoff() > maxPayoffNeighbour
						.getAccumulatedPayoff())
					maxPayoffNeighbour = nei;
			}
			return maxPayoffNeighbour.getStrategy();
		}
	},
	/**
	 * FERMI：从直接邻居学习策略，个体i随机选择一个邻居j后，按照下式决定是否学习该邻居，
	 * 其中Pi是i的本轮收益，Pj是邻居j的本轮收益，i学习后者j的概率为w=1/(1+exp[(pi-pj)/k]),
	 * k描述了环境的噪声因素，刻画了个体的非理性程度。当时k趋近于0时，意味着i具有完全理性――它只会学习高于自身收益的行为。
	 * 而随着k的增加，个体理性程度降低，学习低收益邻居行为的可能性增加
	 */
	FERMI("fermi_learning") {

		float noise = 0.1f; // 噪声暂时设为0.1

		@Override
		public float learn(Individual in) {
			// TODO Auto-generated method stub
			double imitatePosibility = 0;
			final ArrayList<Individual> neighbours = in.getNeighbours();
			if (!neighbours.isEmpty()) {
				Individual neighbour = neighbours.get((int) (RandomUtil
						.nextFloat() * neighbours.size()));
				imitatePosibility = 1 / (1 + Math.exp((in
						.getAccumulatedPayoff() - neighbour
						.getAccumulatedPayoff())
						/ noise));
				if (RandomUtil.nextDouble() <= imitatePosibility) {
					return neighbour.getStrategy();
					// System.out.println(""+this+" learn from \n\t"+neighbour+" with "
					// +imitatePosibility);
				}
			}
			return in.getStrategy();
		}
	},
	INTERACTIVE_FERMI("interactive_fermi") {
		float noise = 0.1f; // 噪声暂时设为0.1

		@Override
		public float learn(Individual in) {
			// TODO Auto-generated method stub
			double imitatePosibility = 0;
			final ArrayList<Individual> neighbours = in.getNeighbours();
			if (!neighbours.isEmpty()) {
				Individual neighbour = null; // neighbour to learn from

				float neighboursPayoff = 0.0f;
				for (Individual nei : neighbours) {
					neighboursPayoff += nei.getAccumulatedPayoff();
				}
				// there are instances where neighboursPayoff is equal to zero.
				// Where this occurs, player x will
				// randomly select one player y from its adjacent neighbors.
				if (neighboursPayoff <= 1.e-6) {
					neighbour = neighbours
							.get((int) (RandomUtil.nextFloat() * neighbours
									.size()));
				} else {
					float chooseRan = neighboursPayoff * RandomUtil.nextFloat();
					float preA = 0;
					float A = 0;
					for (Individual nei : neighbours) {
						A += nei.getAccumulatedPayoff();
						if (preA <= chooseRan && chooseRan <= A) {
							neighbour = nei;
							break;
						}
						preA = A;
					}
				}

				imitatePosibility = 1 / (1 + Math.exp((in
						.getAccumulatedPayoff() - neighbour
						.getAccumulatedPayoff())
						/ noise));
				if (RandomUtil.nextDouble() <= imitatePosibility) {
					return neighbour.getStrategy();
					// System.out.println(""+this+" learn from \n\t"+neighbour+" with "
					// +imitatePosibility);
				}
			}
			return in.getStrategy();
		}
	};

	public String name;

	private LearningPattern(String s) {
		name = s;
	}

	/**
	 * 个体按照该实例所指的学习模式尝试学习新策略
	 * 
	 * @param in
	 *            要进行学习的个体
	 * @return 学习到的新策略
	 */
	public float learn(Individual in) {
		// do nothing
		return in.getStrategy();
	}

	@Override
	public String toString() {
		return name;
	}
}
